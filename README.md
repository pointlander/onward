# Onward
Uses self-entropy/self-attention to learn the first layer in a two layer neural network with a single pass. Self-entropy is calculated as: entropy(softmax(softmax(X*X^T)*X))

## Citations
* [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
* [The Forward-Forward Algorithm: Some Preliminary Investigations](https://arxiv.org/abs/2212.13345)